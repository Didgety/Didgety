{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Didgety/Didgety/blob/main/QuantumGenerativeModels_I_BornMachines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKM9xSVRAMla"
      },
      "source": [
        "# **Quantum Generative Models I: The Quantum Circuit Born Machine**\n",
        "QSC Summer School Elective Day 1: Training quantum models on classical data\n",
        "\\\n",
        "Author:\n",
        "* Kathleen E. Hamilton, ORNL, (hamiltonke@ornl.gov)\n",
        "\n",
        "\n",
        "## Overview\n",
        "The field of quantum machine learning (QML) is rapidly growing, and every day researchers around the world are constructing and publishing on models for:\n",
        "* Supervised learning on classical data (classical features, classical labels, quantum model)\n",
        "* Supervised learning on quantum data (quantum featuers, quantum labels, quantum model)\n",
        "* Unsupervised learning\n",
        "* Semi-supervised learning\n",
        "* Discriminative modeling\n",
        "* Causal modeling\n",
        "* Probabilistic modeling\n",
        "* **Generative modeling**\n",
        "\n",
        "In addition to the plethora of learning tasks, there is so much data to work with and analyze.  \n",
        "\n",
        "In additon to all the learning tasks, associated with each is a broad design space, with many choices available for data encoding, parameterized model construction and extracting information from quantum states.\n",
        "\n",
        "I think this is all great, and the research landscape is ever growing.\n",
        " However, this elective course meets for 2 days and we have 2 hours each day.  My goal is to do the minimum amount of talking, and instead have the majority of our time dedicated to hands-on numerical coding and testing.\n",
        "\n",
        "To that end, I have designed these Colab notebooks to act as a \"Choose Your Own Adventure\" -- there are $3$ Sections planned out as follows:\n",
        "\n",
        "1. Training a generator on discrete value data\n",
        "2. Training a generator on continuous value data\n",
        "3. Training a image denoiser on discrete data\n",
        "\n",
        "\n",
        "Under Sections 1--2 I have provided minimal working examples (MWEs) that rely on many pre-defined functions available in [PennyLane](https://pennylane.ai/).\n",
        "\n",
        "Certain cost functions, examples of numerical datasets and reasonable guesses for hyperparameters are given.  The code in the MWEs should be executable without any modifications.\n",
        "\n",
        "Underneath each MWE there are several questions and options to explore and build upon the MWE -- testing out different cost functions, building your own trainable circuit structure, trying other datasets.  \n",
        "\n",
        "My goal for today is to walk through the MWEs, then you have the option to explore the one that most interests you.  Section 3 requires the most coding, it does not have an immediate MWE. While we will start discussing it during our class time, it may require additional time and is provided as a \"homework\" example, although please note, these are not required.\n",
        "\n",
        "##Background\n",
        "\n",
        "The purpose of this notebook is to work through the construction, training, and evaluation of generative models.  The purpose of this generative model is to produde data examples that are similar to those generated by an unknown stochastic process.  \n",
        "\n",
        "We will mostly consider models that are trained on _unlabeled_ data.  The training dataset $\\mathbf{X}=\\lbrace(x_n)\\rbrace$ consists of $M$-samples of m-length feature vectors $x_i$.  \n",
        "\n",
        "###Classical Models\n",
        "\n",
        "There are many examples of classical generative models: Boltzmann machine, generative adversarial networks, variational autoencoders, flows and diffusion models.  \n",
        "\n",
        "### The Quantum Circuit Born Machine\n",
        "\n",
        "The first quantum generative model we will use is the Quantum Circuit Born Machine (QCBM) [1].  This is a variational model that is trained to fit the marginal distribution of a fixed traininig dataset: $P(\\mathbf{X})$.\n",
        "\n",
        "For the Quantum Circuit Born Machine (QCBM) the datum $\\mathbf{x}$ are binary strings of length $m$, where $m$ is the number of qubits in the circuit model.\n",
        "\n",
        "\n",
        "#### References\n",
        "[1]Benedetti, Marcello, et al. \"A generative modeling approach for benchmarking and training shallow quantum circuits.\" npj Quantum Information 5.1 (2019): 45. https://arxiv.org/abs/1801.07686\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### SOLUTIONS\n",
        "I have a roster of emails for everyone enrolled in this elective.  This QML elective only meets during Week 1 of the Summer School.  I will provide versions of this Colab that has my implementations between Week 1 and Week 2.\n",
        "\n",
        "I hope this elective will be interesting, and that you enjoy these Colabs.  Any questions, concerns, or if you want to discuss interesting use cases of QML please reach out via my email."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn"
      ],
      "metadata": {
        "id": "GR9wPRnexB3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WITougodCJVf"
      },
      "outputs": [],
      "source": [
        "pip install pennylane"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVOI7iMWOaHM"
      },
      "source": [
        "###Class structure\n",
        "The basic QCBM class is used for building a specific parameterized circuit of depth (L), projecting all qubits onto a fixed basis, and using gradient descent to train all parameters in the circuit.\n",
        "\n",
        "The following MWE class definition was pulled from several codebases that I've developed -- inside the class there are several pre-defined functions:\n",
        "* `build_circuit` builds the QCBM circuit with the specified number of layers, and layer type. The examples use the pre-defined templates found in Pennylane, but there is also an option to define a unique user template.\n",
        "* `pdf` returns the distribution over the computational basis states.\n",
        "* `spin_vector` returns the average spin of each qubit in the register $[\\langle Z_0\\rangle, \\langle Z_1\\rangle,\\dots, \\langle Z_{n-1}\\rangle]$ (*not impelemented, needs to be added*\n",
        "* `fit` trains the QCBM to return the optimal `pdf` with a specific cost function and optimizer.\n",
        "\n",
        "While the majority of exercises today will use noiseless simulation, there is the option to test out using PennyLane's `mixed.default` simulator to test the effects of noise.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f92pk0iWadUo"
      },
      "outputs": [],
      "source": [
        "from pennylane import numpy as np\n",
        "import pennylane as qml\n",
        "\n",
        "import itertools as it\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from pennylane.templates import BasicEntanglerLayers,StronglyEntanglingLayers,SimplifiedTwoDesign"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZu7-gMxakuN"
      },
      "source": [
        "# Stage 1: the MWE of a QCBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlNK5B8eCHnA"
      },
      "outputs": [],
      "source": [
        "class QCBM:\n",
        "    '''\n",
        "    Parent class for constructing\n",
        "    Quantum Circuit Born Machine, that uses PennyLane\n",
        "    Inputs:\n",
        "    wires -- number of qubits to build in the circuit\n",
        "    shots -- number of shots to take from the prepared state\n",
        "    layers -- how many layers of a template to add to the circuit (using a template)\n",
        "    template -- which predefined template to use:\n",
        "      'basic_template' = BasicEntanglerLayers,\n",
        "      'strong_tempalte' = StronglyEntanglingLayers,\n",
        "      'two_design_template' = SimplifiedTwoDesign,\n",
        "      'user_defined_template' = user defined function\n",
        "    backend -- 'sim' or 'noisy_sim' for simulator or noisy simulator\n",
        "    **kwargs -- additional parameters for the class\n",
        "    '''\n",
        "\n",
        "\n",
        "    def __init__(self, wires=1,shots=1024,layers=1,backend='sim',**kwargs):\n",
        "        self.dev_wires = [np.array(idx, requires_grad=True) for idx in range(wires)] # the individual device wires\n",
        "        self.wires=wires # number of qubits in the circuit\n",
        "        self.rotation = kwargs.get('rotation',None) #option for the BasicEntanglingLayer template which uses single axis rotations as the parameterized gate\n",
        "        self.layers = layers # number of layers of template to add to the circuit\n",
        "        self.layout = kwargs.get('layout','basic_template') # which template to use\n",
        "        self.add_final_layer=kwargs.get('final_layer',False) # option for adding a final layer of rotations to the circuit\n",
        "        self.backend=backend # options: 'sim' or 'noisy_sim'\n",
        "        self.shots = shots # number of shots to sample from the final state\n",
        "        self.n_iter_no_change=kwargs.get('wait_time',5) # hyperparameter for training that triggers early stopping\n",
        "        self.tol = kwargs.get('tol',1e-5) # hyper parameter for training that triggers early stopping\n",
        "        self.pBF=kwargs.get('pBF',None)  # option for noisy simulation controlling the strength of BitFlip errors prior to measurement\n",
        "        self.pDP=kwargs.get('pDP',None) # option for noisy simulation controlling the strength of Depolarizing noise channel\n",
        "        self.pAD=kwargs.get('pAD',None) # option for noisy simulation controlling the strength of Amplitude Damping noise channel\n",
        "        self.pCE=kwargs.get('pCE',None) # option for noisy simulation controlling the strength of coherent errors\n",
        "        self.prob_vec = True\n",
        "        if self.backend=='sim':\n",
        "            self.device = qml.device(\"default.qubit\", \\\n",
        "                                     wires=self.dev_wires,shots=shots)\n",
        "        elif self.backend=='noisy_sim':\n",
        "            if (self.pDP!=None) or (self.pBF!=None)or (self.pAD!=None):\n",
        "                ##print('you need a noisy device')\n",
        "                self.device = qml.device('default.mixed',\\\n",
        "                                    wires = self.dev_wires,shots=shots)\n",
        "        self.rng=np.random.default_rng(2024)\n",
        "\n",
        "    def write_metadata(self):\n",
        "      \"\"\"Option: Add this functionality\"\"\"\n",
        "      \"\"\" Add code here to initialize a directory to store output files\"\"\"\n",
        "      \"\"\" Example: trained model parameters, loss curves, information about hyperparameters used\"\"\"\n",
        "      raise NotImplementedError\n",
        "\n",
        "    def build_circuit(self,params):\n",
        "      \"\"\" Option: Add some code to check that the correct number of parameters\n",
        "      have been passed to build the circuit \"\"\"\n",
        "      # # #\n",
        "      # Here\n",
        "      # # #\n",
        "      if self.add_final_layer:\n",
        "        unitary_params = params[:-self.wires*2]\n",
        "        final_params = params[-self.wires*2:]\n",
        "      else:\n",
        "        unitary_params = params\n",
        "        final_params = None\n",
        "      \"\"\" Option: The MWE just uses the all zero state as the initial state \"\"\"\n",
        "      # # #\n",
        "      # Add code here to provide the option to use a different initial state\n",
        "      # # #\n",
        "      qml.BasisState(np.zeros(self.wires), wires=self.dev_wires)\n",
        "      if self.layout=='basic_template':\n",
        "        shape = BasicEntanglerLayers.shape(n_layers=self.layers,\\\n",
        "                                                      n_wires=self.wires)\n",
        "        unitary_params=np.asarray(unitary_params).reshape(shape)\n",
        "        BasicEntanglerLayers(unitary_params,rotation=self.rotation,\\\n",
        "                                                        wires = self.dev_wires)\n",
        "      elif self.layout=='strong_template':\n",
        "        shape = StronglyEntanglingLayers.shape(n_layers=self.layers,\\\n",
        "                                                        n_wires=self.wires)\n",
        "        unitary_params=np.asarray(unitary_params).reshape(shape)\n",
        "        StronglyEntanglingLayers(unitary_params, wires = self.dev_wires)\n",
        "      elif self.layout=='two_design_template':\n",
        "        shape = SimplifiedTwoDesign.shape(n_layers=self.layers, \\\n",
        "                                                        n_wires=self.wires)\n",
        "        unitary_params=np.asarray(unitary_params).reshape(shape)\n",
        "        SimplifiedTwoDesign(unitary_params, wires = self.dev_wires)\n",
        "      elif self.layout=='user_defined_template':\n",
        "        # ADD YOUR CODE HERE\n",
        "        # # #\n",
        "        # write a function that can be passed to qml.template or use other\n",
        "        # functionalitiy such as qml.broadcast to build your own parameterized\n",
        "        # circuit design\n",
        "        # # #\n",
        "        raise NotImplementedError\n",
        "      else:\n",
        "        raise ValueError('Please choose a valid layout')\n",
        "      if final_params is not None:\n",
        "        # ADD YOUR CODE HERE\n",
        "        # Ensure that final_params has the correct shape\n",
        "        #\n",
        "        qml.broadcast(unitary=qml.RY, pattern=\"single\", \\\n",
        "                            wires=self.dev_wires, parameters=final_params[0])\n",
        "        qml.broadcast(unitary=qml.RX, pattern=\"single\", \\\n",
        "                      wires=self.dev_wires, parameters=final_params[1])\n",
        "\n",
        "      if self.prob_vec:\n",
        "          if self.pBF!=None:\n",
        "              for idx in self.dev_wires:\n",
        "                  qml.BitFlip(self.pBF, wires=self.dev_wires[int(idx)])\n",
        "              return qml.probs(wires=self.dev_wires)\n",
        "          return qml.probs(wires=self.dev_wires)\n",
        "      else:\n",
        "          return [qml.expval(qml.PauliZ(i)) for i in self.dev_wires]\n",
        "    def draw_circuit(self,*args, **kwds):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def pdf(self,params):\n",
        "        '''\n",
        "        pennylane returns the probabilities in an OrderedDict - no need for sorting the values\n",
        "        '''\n",
        "        self.prob_vec=True\n",
        "        qnode_ = qml.QNode(self.build_circuit, self.device)\n",
        "        Q = qnode_(params)\n",
        "        return Q\n",
        "\n",
        "    def spin_vector(self,*args, **kwds):\n",
        "      \"\"\" ADD CODE HERE \"\"\"\n",
        "      # Add code so that the output of the QCBM is a vector of [<Z0>, <Z1>,...]\n",
        "      raise NotImplementedError\n",
        "\n",
        "    def sample(self,*args,**kwds):\n",
        "      \"\"\" ADD CODE HERE\"\"\"\n",
        "      # Add code to prepare the pdf and draw N samples\n",
        "      # return samples as bitstrings s\n",
        "      raise NotImplementedError\n",
        "\n",
        "    def fit(self,startval,opt_method,cost_fcn,target,nsteps=10,alpha=0.05):\n",
        "      \"\"\" ADD CODE HERE \"\"\"\n",
        "      # Add code to handle the case that an argument is missing\n",
        "\n",
        "      opt = opt_method(stepsize=alpha)\n",
        "\n",
        "      params = startval.copy()\n",
        "\n",
        "      loss = {}\n",
        "      loss[0]=cost_fcn(target,self.pdf(params))\n",
        "      print('initial loss: ',loss[0])\n",
        "\n",
        "      stored_params = {}\n",
        "      stored_params[0]=params.copy()\n",
        "\n",
        "      for i in range(nsteps):\n",
        "          # Update the circuit parameters\n",
        "          params,_loss = opt.step_and_cost(lambda v: cost_fcn(target,self.pdf(v)), params)\n",
        "          # Option: Does this print out too much during training?  add a verbose keyword to toggle that on/off\n",
        "          if i%2==0:\n",
        "              print(i+1,_loss)\n",
        "          loss[i+1]=_loss.copy()\n",
        "          stored_params[i+1]=params.copy()\n",
        "      return params,loss,stored_params\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujlwG1E1g0IE"
      },
      "source": [
        "Check out the MWE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1za0ckPg4cW"
      },
      "outputs": [],
      "source": [
        "n_wires = 2\n",
        "n_layers = 2\n",
        "n_shots = 256\n",
        "my_qcbm = QCBM(wires=n_wires,layers=n_layers,shots=n_shots,backend='sim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FAGnjZFh5DW"
      },
      "outputs": [],
      "source": [
        "my_qcbm.__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX5ENRptjuVl"
      },
      "outputs": [],
      "source": [
        "thetas = np.random.uniform(size=(n_layers,n_wires))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_x3cfBDhHlD"
      },
      "outputs": [],
      "source": [
        "temp_qnode = qml.QNode(my_qcbm.build_circuit, my_qcbm.device)\n",
        "qml.draw_mpl(temp_qnode,decimals=3,expansion_strategy='device')(thetas)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8CrHJ8DlFan"
      },
      "outputs": [],
      "source": [
        "my_qcbm.pdf(thetas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmkr-vbXlI87"
      },
      "outputs": [],
      "source": [
        "f, ax = plt.subplots(figsize = (10,6))\n",
        "sns.barplot(my_qcbm.pdf(thetas),legend=False,ax=ax)\n",
        "# option:  adjust the Matplotlib axes objects so that the x labels are bitstrings '00', '01', '10', '11'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddh7TXKLHHR7"
      },
      "source": [
        "## Density Modeling\n",
        "\n",
        "In the first example, we are going to train the MWE QCBM to fit the probability density function of a dataset composed of one-dimensional features.  \n",
        "\n",
        "In the MWE built on 2 wires, the output from `pdf(thetas)` is a distribution over $2^n =4$ binary bitstrings.  \n",
        "\n",
        "Let's try to train this to prepare a specific distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSmv4SDNmWoD"
      },
      "outputs": [],
      "source": [
        "#Options for targets (choose one, comment out the rest)\n",
        "default_rng = np.random.default_rng(2024) # random number generator\n",
        "target = np.asarray([0, 0, 1, 0]) #prepare a specific bitstring with probability 100\n",
        "#target = [0.5, 0, 0, 0.5] # prepare two bitstrings, equally probable\n",
        "#target = [0.5, 0.5, 0, 0] # prepare two other bitstrings, equal probability\n",
        "#target = [0, 0.25,0.75,0] # prepare two bitstrings, unequally\n",
        "#target = default_rng.random(size=4) # random target -  CAUTION!! make sure this is a normalized vector before using it!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p21UYDGJni_g"
      },
      "source": [
        "Now we need a cost function.  I am choosing to use a cost function that will be minimized by gradient descent, so an ideal solution will be the set of parameters\n",
        "$$\\widehat{\\theta} = \\mathrm{argmin}_{\\theta}(\\ell(\\theta))$$.\n",
        "\n",
        " Here are some examples, the Kullback-Leibler Divergence,  or the cosine similarity.\n",
        "\n",
        "The KLD quantifies the similarity between two distributions $\\mathbb{P}, \\mathbb{Q}_{\\theta}$ where $\\mathbb{P}$, is the fixed target chosen above and $\\mathbb{Q}_{\\theta}$ is the distribution produced by the QCBM when `pdf(theta)` is called.\n",
        "\n",
        "The KLD is defined as,\n",
        "$$ \\ell(\\theta,\\mathbb{P}) = KLD(\\mathbb{P},\\mathbb{Q}_{\\theta}) = \\sum_{i} p(x_i)\\log{\\frac{p(x_i)}{q_{\\theta}(x_i)}}$$\n",
        "where an overall constant term (the Shannon entropy of the target) was omitted and the summation is over the individual bitstring probabilities $p(x_i)$ and $q(x_i)$.\n",
        "\n",
        "The cosine similarity is more general, and compute the similarity between two vectors $\\mathbb{P}, \\mathbb{Q}_{\\theta}$ but does not require that the two vectors are normalized, but it does require that they are _normalizable_ (i.e. $|\\mathbb{P}| \\neq 0$ and $|\\mathbb{Q}|\\neq 0$).\n",
        "\n",
        "$$ \\ell(\\theta,\\mathbb{P}) = 1 - \\frac{\\mathbb{P}\\cdot \\mathbb{Q}_{\\theta}}{|\\mathbb{P}||\\mathbb{Q}_{\\theta}|} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "869vL6dqnPOJ"
      },
      "outputs": [],
      "source": [
        "def kl_div(p,q):\n",
        "    # if q = 0 that is going to cause problems\n",
        "    # add a small epsilon to q to avoid np.nan or np.inf results\n",
        "    from itertools import combinations\n",
        "    qk = np.asarray(q)\n",
        "    ck = np.broadcast(p, q)\n",
        "\n",
        "    vec = [u*np.log(u/v) if (u>0 and v>0) else 0 if (u == 0 and v>=0) else np.log(1e10) for (u,v) in ck]\n",
        "    S = np.sum(vec)\n",
        "    return S\n",
        "\n",
        "def cosine_distance(p,q):\n",
        "    #p = target\n",
        "    pnorm = np.sqrt(np.sum([x**2 for x in p]))\n",
        "\n",
        "    #q = pdf(params)\n",
        "    qnorm = np.sqrt(np.sum([x**2 for x in q]))\n",
        "    return 1.0 - (p@q)/(pnorm*qnorm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StF5trHmsz6G"
      },
      "outputs": [],
      "source": [
        "trained_model,loss_curve,all_params = my_qcbm.fit(thetas,qml.AdamOptimizer,cosine_distance,target,nsteps=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YA4BEs4viFs"
      },
      "source": [
        "the function `.fit(...)` returns 3 outputs:  the parameters of the final trained parameter, a dictionary storing the loss at each optimization step, and a dictionary all the parameters at each optimization step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1IIX5ujs2ln"
      },
      "outputs": [],
      "source": [
        "output = pd.DataFrame(my_qcbm.pdf(thetas))\n",
        "output.columns = ['initial']\n",
        "output['fitted'] = my_qcbm.pdf(trained_model)\n",
        "output['target'] = target\n",
        "f, ax = plt.subplots(figsize = (10,6))\n",
        "output.plot(kind='bar',legend=True,ax=ax,rot=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Hxx9544uQKf"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(loss_curve.values(),legend=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2nCC-P0A54n"
      },
      "source": [
        "# Section 1: Training a QCBM on discrete data\n",
        "\n",
        "This MWE showed how a 2 qubit QCBM with a small number of parameters can fit a target density funcion.  Let's review the main componenets of the QCBM:\n",
        "\n",
        "\n",
        "\n",
        "1.   The initial state is fixed, in the example above it was always the $|0\\rangle^{\\otimes n}$ state.   The `build_circuit()` function can be modified to take a different input state (ex: $|+\\rangle^{\\otimes n}$ or a binary state)\n",
        "2.   When `pdf()` is called, the parameterized unitary is applied to the initial state and the prepared state is then projected onto a fixed basis\n",
        "3.  From this distributions, you can draw individual samples\n",
        "\n",
        "\n",
        "In this Section we look at a larger example (4 qubits) using and a common benchmark dataset known as the Bars and Stripes image set.\n",
        "\n",
        "The Bars and Stripes image set is comprised of black and white images that depict either \"bars\" (columns are the same color all black or all white), or \"stripes\" (rows are the same color all black or all white).\n",
        "\n",
        "For this elective the images are restricted to be square with $n$ pixels per side-- $(n \\times n)$ total pixels.  Producing valid images requires $(n \\times n)$ qubits.\n",
        "\n",
        "The goal of this Section is to train a generative model to return data from a set defined from a fixed set of rules.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r32qRfjmjvAf"
      },
      "outputs": [],
      "source": [
        "def bars_and_stripes(n):\n",
        "  ''' generate bars and stripes images of size n x n and return as binary strings'''\n",
        "  output = np.zeros(2**(n**2))\n",
        "\n",
        "  output[0]=1 #all zero bitstring is valid image\n",
        "  output[-1]=1 # all one bitstring is valid image\n",
        "\n",
        "  #generate all valid single stripe and single bar images\n",
        "  for idx in range(n):\n",
        "    temp = np.zeros((n,n),dtype=int)\n",
        "    temp[idx]=1\n",
        "    res = 0\n",
        "    for ele in temp.ravel():\n",
        "        res = (res << 1) | ele\n",
        "    output[res]=1\n",
        "    temp = np.zeros((n,n),dtype=int)\n",
        "    temp.T[idx]=1\n",
        "    res = 0\n",
        "    for ele in temp.ravel():\n",
        "        res = (res << 1) | ele\n",
        "    output[res]=1\n",
        "  for idx in range(2,n):\n",
        "    for subset in it.combinations(list(range(n)), idx):\n",
        "      temp = np.zeros((n,n),dtype=int)\n",
        "      temp[list(subset)]=1\n",
        "      res = 0\n",
        "      for ele in temp.ravel():\n",
        "          res = (res << 1) | ele\n",
        "      output[res]=1\n",
        "      temp = np.zeros((n,n),dtype=int)\n",
        "      temp.T[list(subset)]=1\n",
        "      res = 0\n",
        "      for ele in temp.ravel():\n",
        "          res = (res << 1) | ele\n",
        "      output[res]=1\n",
        "  return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW4UNzOp1jMr"
      },
      "outputs": [],
      "source": [
        "def plot_bars_and_stripes(n):\n",
        "\n",
        "  valid_images = np.where(bars_and_stripes(n)>0)[0]\n",
        "\n",
        "\n",
        "  w = len(valid_images)//4\n",
        "  h = 4\n",
        "  fig, axs = plt.subplots(nrows=h, ncols=w,figsize=(10,10))\n",
        "\n",
        "  for idx, ax in enumerate(axs.flat):\n",
        "    ax.imshow(np.array([int(x) for x in np.binary_repr(valid_images[idx],width=n**2)]).reshape((n,n)).numpy())\n",
        "    # Major ticks\n",
        "    ax.set_xticks(np.arange(0, n, 1))\n",
        "    ax.set_yticks(np.arange(0, n, 1))\n",
        "\n",
        "    # Labels for major ticks\n",
        "    #ax.set_xticklabels(np.arange(1, n+1, 1))\n",
        "    #ax.set_yticklabels(np.arange(1, n+1, 1))\n",
        "\n",
        "    # Minor ticks\n",
        "    ax.set_xticks(np.arange(-.5, n, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, n, 1), minor=True)\n",
        "\n",
        "    # Gridlines based on minor ticks\n",
        "    ax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n",
        "\n",
        "    # Remove minor ticks\n",
        "    ax.tick_params(which='both', bottom=False, left=False)\n",
        "    ax.tick_params(which='major', bottom=False, left=False)\n",
        "    fig.tight_layout()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mskSbDx49N-k"
      },
      "outputs": [],
      "source": [
        "plot_bars_and_stripes(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPQ__rhbHL8b"
      },
      "source": [
        "The function`bars_and_stripes(n)` generates valid images of size $n^2$, which requires $N = 2*n$ qubits.\n",
        "\n",
        "For a MWE we will use `n = 2` to fit the Bars and Stripes distribution, where the trained model should produce valid images on $2 \\times 2$ pixels.  The QCBM needs $N = 4$ qubits.  \n",
        "\n",
        "The other model parameters are (arbitrarily) chosen as $L = 2$ (layers) and the QCBM is sampled with $n_s = 128$ shots.  The optimizer used is Adam and the learning rate is set (again arbitrarily) to $\\alpha = 0.5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dAOkNilAmRR"
      },
      "outputs": [],
      "source": [
        "target = bars_and_stripes(2)\n",
        "target = target/np.sum(target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeAdO0Bc3vUk"
      },
      "outputs": [],
      "source": [
        "n_wires = 4\n",
        "n_layers = 2\n",
        "n_shots = 128\n",
        "my_qcbm = QCBM(wires=n_wires,layers=n_layers,shots=n_shots,backend='sim')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training starts from a random unitary --the QCBM parameterized ansatz evaluated at a random choice of parameters.  \n",
        "\n",
        "This MWE starts at the all zero vector $\\theta_0 = [0, 0, \\dots, 0]$.  The number of parameters needed is determined by the layer template you are using, and if the final layer of rotations is included or not."
      ],
      "metadata": {
        "id": "hhBkQRoUhEwm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SgPkjJuBueP"
      },
      "outputs": [],
      "source": [
        "thetas = np.zeros(n_layers*n_wires) #When building the BasicEntangler layout on n_wires, each layer has (n_wires) parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw3jpw7TA0HS"
      },
      "outputs": [],
      "source": [
        "trained_model,loss_curve,all_params = my_qcbm.fit(thetas,qml.AdamOptimizer,kl_div,target,nsteps=64,alpha=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('here are the final trained parameters')\n",
        "print(trained_model)"
      ],
      "metadata": {
        "id": "r1djDnu1iG3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BJs5W_zBhT2"
      },
      "outputs": [],
      "source": [
        "output = pd.DataFrame(my_qcbm.pdf(thetas))\n",
        "output.columns = ['initial']\n",
        "output['fitted'] = my_qcbm.pdf(trained_model)\n",
        "output['target'] = target\n",
        "f, ax = plt.subplots(figsize = (10,6))\n",
        "output.plot(kind='bar',legend=True,ax=ax,rot=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb_uIOu-CRFx"
      },
      "outputs": [],
      "source": [
        "print('here is the loss curve during training')\n",
        "ax=sns.lineplot(loss_curve.values(),legend=False)\n",
        "ax.set(yscale='log')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('here are all the parameter updates at each step of the training')\n",
        "all_params"
      ],
      "metadata": {
        "id": "qy2GkO-fiT0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bpp5ww-HT7T"
      },
      "source": [
        "### (Optional Tests to Explore):\n",
        "\n",
        "1. Change the number of shots used to sample from the QCBM (Caution: don't just change the `shots`, you also need to update the `device` properties)\n",
        "2. Add functionality to return a valid bar or stripe from the trained QCBM, not just the `pdf()`\n",
        "3. There may be large jumps in the loss during minimization, is that only dependent on the number of shots, or is there any interesting changes in the parameters during training?  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-JspUYaFYWa"
      },
      "source": [
        "# Section 2: Training a Generative model on continuous valued data\n",
        "\n",
        "This is a modification of how the target distribution is constructed.  In the previous section, the Bars and Stripes dataset was defined with a discrete set of valid outputs (images) to generate.\n",
        "\n",
        "The target is constructed from continuous-valued data distribution.  \n",
        "\n",
        "To map continuous-valued data to a discrete distribution, the values are binned and binary labels are assigned to each bin.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vn1uu8zvCVqc"
      },
      "outputs": [],
      "source": [
        "from matplotlib.colors import LogNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3Zcm_PY26Bt"
      },
      "outputs": [],
      "source": [
        "n_samples=2000000\n",
        "mu0 = [1,2.5]\n",
        "cov0 = [[0.5, 0], [0, 0.5]]\n",
        "\n",
        "mu1 = [-4,-5]\n",
        "cov1 = [[2, .1], [1, 0.4]]\n",
        "\n",
        "\n",
        "mu2 = [-2,-1.3]\n",
        "cov2 = [[-1, .7], [.1, 0.4]]\n",
        "\n",
        "X0 = np.random.multivariate_normal(mu0, cov0, n_samples)\n",
        "X1 = np.random.multivariate_normal(mu1, cov1, n_samples)\n",
        "X2 = np.random.multivariate_normal(mu2, cov2, n_samples)\n",
        "X = np.concatenate([X0, X1,X2])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CT7XvC-UHuzt"
      },
      "outputs": [],
      "source": [
        "H,x_edges,y_edges = np.histogram2d(X[:,0], X[:,1], bins=64,density=True)\n",
        "#  the number of bins per dimension requires log(bins) qubits\n",
        "#  with bins=64 we would need 6 qubits (12 total) to map the entire pdf\n",
        "#  with bins = 128 we would need 7 qubits (14 total) to represent the entire pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b8sPx0i40FO"
      },
      "outputs": [],
      "source": [
        "plt.imshow(H)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this MWE the target distribution will be discretized with few qubits ($N = 3$) per dimension (to reduce the total number of needed qubits)."
      ],
      "metadata": {
        "id": "Gwtpy00SrLGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-mEWJqtZ5JCP"
      },
      "outputs": [],
      "source": [
        "n = 3 # qubits per dimension\n",
        "\n",
        "H,x_edges,y_edges = np.histogram2d(X[:,0], X[:,1], bins=2**n,density=True)\n",
        "\n",
        "y0_labels = [np.binary_repr(x,n) for x in range(2**n)]\n",
        "y1_labels = [np.binary_repr(x,n) for x in range(2**n)]\n",
        "\n",
        "target = {}\n",
        "for index in np.ndindex((2**n,2**n)):\n",
        "    target[y0_labels[index[0]]+y1_labels[index[1]]]=H.T[index]\n",
        "\n",
        "QcountSorted = collections.OrderedDict(sorted(target.items()))\n",
        "P_target = [x for x in QcountSorted.values()]\n",
        "P_target = np.asarray(P_target)/np.sum(P_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KENh3V1VZbh"
      },
      "outputs": [],
      "source": [
        "plt.matshow(H+1e-10, norm=LogNorm(vmin=H.min(), vmax=H.max()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0WO32tIWam0"
      },
      "outputs": [],
      "source": [
        "plt.plot(P_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv07SzKwa5ub"
      },
      "outputs": [],
      "source": [
        "n_wires = 2*n\n",
        "n_layers = 8\n",
        "n_shots = 5000\n",
        "my_qcbm = QCBM(wires=n_wires,layers=n_layers,shots=n_shots,backend='sim')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B5dtBPGbIpR"
      },
      "outputs": [],
      "source": [
        "thetas = (np.pi/2.)*np.random.random(n_layers*n_wires+2*n_wires*my_qcbm.add_final_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The call to `.fit()` is the same as the previous section, however with a larger QCBM and the need for more decimal places in the target, the number of shots is increased to 5000"
      ],
      "metadata": {
        "id": "Qcx9u_86rcRO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wtATPORbMYT"
      },
      "outputs": [],
      "source": [
        "trained_model,loss_curve,all_params = my_qcbm.fit(thetas,qml.AdamOptimizer,kl_div,P_target,nsteps=64,alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first example used a large learning rate $\\alpha = 0.5$, and during training there are some clear indications this may be too large."
      ],
      "metadata": {
        "id": "4kluhIn2rr9p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h5vm4_qbke-"
      },
      "outputs": [],
      "source": [
        "ax=sns.lineplot(loss_curve.values(),legend=False)\n",
        "ax.set(yscale='log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdKxA6Jgboo-"
      },
      "outputs": [],
      "source": [
        "trained_model,loss_curve,all_params = my_qcbm.fit(thetas,qml.AdamOptimizer,kl_div,P_target,nsteps=64,alpha=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reducing the learning rate improves the behavior of the gradient descent optimizer."
      ],
      "metadata": {
        "id": "-hj_1sC2r0bF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uzx1CDZ1eWPV"
      },
      "outputs": [],
      "source": [
        "ax=sns.lineplot(loss_curve.values(),legend=False)\n",
        "ax.set(yscale='log')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When testing different training parameters, try adding the final layer of rotations.\n",
        "\n",
        "Remember, in `build_circuit(...)` there is an additional line of code that is needed to ensure that the parameters have the correct format for the way this layer is currently implemented.\n",
        "\n",
        "If adding the final layer of rotations, then the QCBM has an additional $2n$ parameters."
      ],
      "metadata": {
        "id": "ls-QDrWFjI5s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mf_98UPnfkAX"
      },
      "outputs": [],
      "source": [
        "n_shots = 64000\n",
        "my_qcbm = QCBM(wires=n_wires,layers=n_layers,shots=n_shots,backend='sim',final_layer=True)\n",
        "thetas = (np.pi/2.)*np.random.random(n_layers*n_wires+2*n_wires*my_qcbm.add_final_layer)-np.pi/4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxmA1WrqfuX2"
      },
      "outputs": [],
      "source": [
        "trained_model,loss_curve,all_params = my_qcbm.fit(thetas,qml.AdamOptimizer,kl_div,P_target,nsteps=64,alpha=0.05)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Furthermore, increasing the number of shots also improves performance."
      ],
      "metadata": {
        "id": "XeZXdgu1r9Dc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIeOd_mrfxNY"
      },
      "outputs": [],
      "source": [
        "ax=sns.lineplot(loss_curve.values(),legend=False)\n",
        "ax.set(yscale='log')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing the fitted distribution to the target, there is still room for improvement."
      ],
      "metadata": {
        "id": "uGcoLyjdsV1n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV8roM5tbQOr"
      },
      "outputs": [],
      "source": [
        "output = pd.DataFrame(my_qcbm.pdf(thetas))\n",
        "output.columns = ['initial']\n",
        "output['fitted'] = my_qcbm.pdf(trained_model)\n",
        "output['target'] = P_target\n",
        "f, ax = plt.subplots(figsize = (16,6))\n",
        "ax.set(yscale='log')\n",
        "output.plot(kind='bar',legend=True,ax=ax,rot=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loH2uG0texN3"
      },
      "outputs": [],
      "source": [
        "H_pred = my_qcbm.pdf(trained_model).reshape((2**n,2**n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14o_U-uGmM0J"
      },
      "outputs": [],
      "source": [
        "plt.matshow(H_pred+1e-10, norm=LogNorm(vmin=H.min(), vmax=H.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional (Next steps)\n",
        "\n",
        "Going beyond this MWE, how can the fitting be improved?  This is a good example of the need for hyperparameter optimization, as well as determining the optimal ansatz design.\n",
        "\n",
        "1) Increasing the depth, shots, or number of optimization steps\n",
        "\\\n",
        "2) decreasing the learning rate\n",
        "\\\n",
        "3) Changing the cost function\n",
        "\\\n",
        "4) Adjusting the mesh spacing used to discretize the target"
      ],
      "metadata": {
        "id": "U1FaQ67Hsddm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIKgSQz6sOA5"
      },
      "source": [
        "## Section 3: Training a generative model with input\n",
        "\n",
        "So far, the QCBMs trained in this network take a fixed input state and return samples drawn from a fixed distribution.  But these samples are returned by uniform sampling.\n",
        "What if we want to provide partial information or an input feature to the model?  For example, one use case for generative models is *denoising*.  A trained model is used to extract a \"true signal\" from a corrupted input.  For this MWE we are going to return a valid image from the Bars and Stripes dataset that has been corrupted from some noise.\n",
        "\n",
        "To explore this, need a way to encode input vectors $x$ into the model (in `build_circuit(...)`), and the training worfklow needs to be modified (in `fit(...)`).  Before making changes, let's make a copy of the `QCBM` class and make modify it.\n",
        "\n",
        "First, in `build_circuit(...)` let's adapt the generator design used in Style-GAN models [2].  This was used with adversarial training in [3], we're looking at non-adversarial training (no discriminator).  The main component that needs to be added, is the ability to encode latent representations of input features, periodically in the unitary circuit.\n",
        "\n",
        "A full Style-GAN model will first use a deep neural network to define that latent representation and the style-generator encodes these latent features into the gnereator. In this MWE the latent space is assumed to be the one spanned by one-hot encoding vectors. Given an image (or corrupted image) of $N$ pixels, each corrupted pixed is encoded as a length $N$ zero vectorand only the entry for pixel [i] is non-zero,\n",
        "$$ v[i] = p[i] $$\n",
        "the $i$-th entry of the vector is equal to the pixel value.\n",
        "\n",
        "Recall from the description of the Bars and Stripes dataset above, the pixels are mapped to binary variables $\\lbrace 0, 1\\rbrace$.  An ideal image (without any corruption) consists of $N$ binary variables, which are now individually encoded as length-$N$ vectors.  \n",
        "\n",
        "Example:  the $i=2$ pixel in a $4$ pixel image is white $p[i]=1$.  The vector representation of this pixel only is $[0, 0, 1, 0]$.\n",
        "\n",
        "Example:   the $i=0$ pixel in a $4$ pixel image has been corrupted, and is $0.5$.  The vector representation of this pixel only is $[0, 0.5, 0, 0]$.\n",
        "\n",
        "References:\n",
        "\\\n",
        "[2] Karras, Tero, Samuli Laine, and Timo Aila. \"A style-based generator architecture for generative adversarial networks.\" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019. [StyleGAN CVPR paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Karras_A_Style-Based_Generator_Architecture_for_Generative_Adversarial_Networks_CVPR_2019_paper.html)\n",
        "\\\n",
        "[3]Bravo-Prieto, Carlos, et al. \"Style-based quantum generative adversarial networks for Monte Carlo events.\" Quantum 6 (2022): 777. [StyleQGAN](https://quantum-journal.org/papers/q-2022-08-17-777/)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoyjQPMYz89y"
      },
      "outputs": [],
      "source": [
        "def plot_noisy_bars_and_stripes(n,p):\n",
        "  ''' plot noisy bars and stripes images of size n x n\n",
        "  where a pixel has a probability (p) of being corrupted'''\n",
        "  valid_images = np.where(bars_and_stripes(n)>0)[0]\n",
        "\n",
        "\n",
        "  w = len(valid_images)//4\n",
        "  h = 4\n",
        "  fig, axs = plt.subplots(nrows=h, ncols=w,figsize=(10,10))\n",
        "\n",
        "  for idx, ax in enumerate(axs.flat):\n",
        "    noiseless_image = [int(x) for x in np.binary_repr(valid_images[idx],width=n**2)]\n",
        "    # TODO: how are the images corrupted:  Gaussian noise?  are random pixels erased?\n",
        "    # ADD CODE HERE\n",
        "    # define a random vector that corrupts a noiseless valid image\n",
        "    noisy_image = ...\n",
        "    ax.imshow(np.array(noisy_image).reshape((n,n)).numpy(),cmap='coolwarm')\n",
        "    # Major ticks\n",
        "    ax.set_xticks(np.arange(0, n, 1))\n",
        "    ax.set_yticks(np.arange(0, n, 1))\n",
        "\n",
        "    # Minor ticks\n",
        "    ax.set_xticks(np.arange(-.5, n, 1), minor=True)\n",
        "    ax.set_yticks(np.arange(-.5, n, 1), minor=True)\n",
        "\n",
        "    # Gridlines based on minor ticks\n",
        "    ax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n",
        "\n",
        "    # Remove minor ticks\n",
        "    ax.tick_params(which='both', bottom=False, left=False)\n",
        "    ax.tick_params(which='major', bottom=False, left=False)\n",
        "    fig.tight_layout()\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_noisy_bars_and_stripes(3,0.001)"
      ],
      "metadata": {
        "id": "3AlSKlcLqCuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first approach to denoising is to use training data generated by $m$ noisy observations of a **known** image (i.e. we have access the ground truth).  So training a denoising model is a supervised learning task.\n",
        "\n",
        "Before the `fit(...)` function is modified we need to generate the training data.  For this MWE we only use one randomly chosen image from the Bars and Stripes dataset, then, depending on our noise model, we generate $m$ noisy versions.\n",
        "\n",
        "The training dataset will be composed as $X = \\lbrace \\widetilde{y}_i,y\\rbrace$.  Each corrupted (noisy) example ($\\widetilde{y}_i$) is labeled by the known ground truth $y$."
      ],
      "metadata": {
        "id": "_-IFDHXVoRUd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz6pzRYS7_sR"
      },
      "outputs": [],
      "source": [
        "def generate_multiple_noisy_samples(n,m,p):\n",
        "  ''' randomly generate an image from the Bar and Stripe dataset with pixel noise\n",
        "  randomly corrupted with noise of strength p'''\n",
        "  valid_images = np.where(bars_and_stripes(n)>0)[0]\n",
        "  random_index = np.random.choice(len(valid_images))\n",
        "  noiseless_image = np.array([int(x) for x in np.binary_repr(valid_images[random_index],width=n**2)])\n",
        "  output = []\n",
        "  for idx in range(m):\n",
        "    # TODO -- add code to implement a particular noise model and corrupt the image\n",
        "    # Ensure that unique corrupted image can be produced\n",
        "    noise = ...\n",
        "    noisy_image = ...\n",
        "    output.append(noisy_image.numpy())\n",
        "  output.append(noiseless_image) # the ground truth is appended at the end\n",
        "  return np.array(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4WVCcAz8qMd"
      },
      "outputs": [],
      "source": [
        "def build_noisy_clean_pairs(noisy_samples):\n",
        "  X = []\n",
        "  y = []\n",
        "  for idx in range(noisy_samples.shape[0]-1):\n",
        "    X.append(noisy_samples[idx]) #corrupted image is the feature\n",
        "    y.append(noisy_samples[-1]) #ground truth is the label\n",
        "  return np.asarray(X),np.asarray(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BEG_-ff9a0h"
      },
      "outputs": [],
      "source": [
        "def build_training_pairs(noisy_samples):\n",
        "  X = []\n",
        "  y = []\n",
        "  for subset in it.combinations(list(range(noisy_samples.shape[0])), 2):\n",
        "    X.append(noisy_samples[subset[0]])\n",
        "    y.append(noisy_samples[subset[1]])\n",
        "  return np.asarray(X),np.asarray(y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTMlzbMr-LjU"
      },
      "outputs": [],
      "source": [
        "# verify that the training pairs are correctly formatted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The `StyleQCBM` class\n",
        "\n",
        "**This Section does not have a MWE -- the QCBM preparation may work, but it may not implement the correct feature encoding**\n",
        "\n",
        "This is a copy (literaly copy-paste) of the `QCBM` class above, this is the code that will be adapted to implement and train a denoising model.\n",
        "\n",
        "Modifications needed:\n",
        "\n",
        "1) `build_circuit(...)` now takes as a keyword `x` an input feature.  The function must be updated to ensure that these feature is used during the state preparation.\n",
        "\n",
        "2) the function used to extract output, either `pdf(...)` or `spin_vector(...)` must also take `x` as a feature vector\n",
        "\n",
        "3) the cost function is now a method of the class, an example of the mean square error loss is given, and it takes `x` (feature) and `y` (label)\n",
        "\n",
        "4) the `fit(...)` function must be updated to implement supervised learning.  A skeleton is given, however it assumes that the modifications 1--3 have been implemented"
      ],
      "metadata": {
        "id": "qD-9w2YSqLSN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNJb_UogmSM4"
      },
      "outputs": [],
      "source": [
        "class StyleQCBM:\n",
        "    '''\n",
        "    Parent class for constructing\n",
        "    a style-based (inspired) Quantum Circuit Born Machine, that uses PennyLane\n",
        "    Inputs:\n",
        "    wires -- number of qubits to build in the circuit\n",
        "    shots -- number of shots to take from the prepared state\n",
        "    layers -- how many layers of a template to add to the circuit (using a template)\n",
        "    template -- which predefined template to use:\n",
        "      'basic_template' = BasicEntanglerLayers,\n",
        "      'strong_tempalte' = StronglyEntanglingLayers,\n",
        "      'two_design_template' = SimplifiedTwoDesign,\n",
        "      'user_defined_template' = user defined function\n",
        "    backend -- 'sim' or 'noisy_sim' for simulator or noisy simulator\n",
        "    **kwargs -- additional parameters for the class\n",
        "    '''\n",
        "\n",
        "\n",
        "    def __init__(self, wires=1,shots=1024,layers=1,backend='sim',**kwargs):\n",
        "        self.dev_wires = [np.array(idx, requires_grad=True) for idx in range(wires)] # the individual device wires\n",
        "        self.wires=wires # number of qubits in the circuit\n",
        "        self.rotation = kwargs.get('rotation',None) #option for the BasicEntanglingLayer template which uses single axis rotations as the parameterized gate\n",
        "        self.layers = layers # number of layers of template to add to the circuit\n",
        "        self.layout = kwargs.get('layout','basic_template') # which template to use\n",
        "        self.add_final_layer=kwargs.get('final_layer',False) # option for adding a final layer of rotations to the circuit\n",
        "        self.backend=backend # options: 'sim' or 'noisy_sim'\n",
        "        self.shots = shots # number of shots to sample from the final state\n",
        "        self.n_iter_no_change=kwargs.get('wait_time',5) # hyperparameter for training that triggers early stopping\n",
        "        self.tol = kwargs.get('tol',1e-5) # hyper parameter for training that triggers early stopping\n",
        "        self.pBF=kwargs.get('pBF',None)  # option for noisy simulation controlling the strength of BitFlip errors prior to measurement\n",
        "        self.pDP=kwargs.get('pDP',None) # option for noisy simulation controlling the strength of Depolarizing noise channel\n",
        "        self.pAD=kwargs.get('pAD',None) # option for noisy simulation controlling the strength of Amplitude Damping noise channel\n",
        "        self.pCE=kwargs.get('pCE',None) # option for noisy simulation controlling the strength of coherent errors\n",
        "        self.prob_vec = True\n",
        "        if self.backend=='sim':\n",
        "            self.device = qml.device(\"default.qubit\", \\\n",
        "                                     wires=self.dev_wires,shots=shots)\n",
        "        elif self.backend=='noisy_sim':\n",
        "            if (self.pDP!=None) or (self.pBF!=None)or (self.pAD!=None):\n",
        "                ##print('you need a noisy device')\n",
        "                self.device = qml.device('default.mixed',\\\n",
        "                                    wires = self.dev_wires,shots=shots)\n",
        "        self.rng=np.random.default_rng(2024)\n",
        "\n",
        "    def write_metadata(self):\n",
        "      \"\"\"Option: Add this functionality\"\"\"\n",
        "      \"\"\" Add code here to initialize a directory to store output files\"\"\"\n",
        "      \"\"\" Example: trained model parameters, loss curves, information about hyperparameters used\"\"\"\n",
        "      raise NotImplementedError\n",
        "\n",
        "    def build_circuit(self,params,x=None):\n",
        "      \"\"\" Option: Add some code to check that the correct number of parameters\n",
        "      have been passed to build the circuit \"\"\"\n",
        "      # # #\n",
        "      # Here\n",
        "      # # #\n",
        "      if self.add_final_layer:\n",
        "        unitary_params = params[:-self.wires*2]\n",
        "        final_params = params[-self.wires*2:]\n",
        "      else:\n",
        "        unitary_params = params\n",
        "        final_params = None\n",
        "      \"\"\" TODO Modify build_circuit construction  \"\"\"\n",
        "      # # #\n",
        "      # Add/modify the code here to encode the feature (x)\n",
        "      # the feature (x) is assumed to be the corrupted image\n",
        "      # need to add code to 1) convert each pixel to a one-hot encoded vector\n",
        "      # and 2) encode these pixel vectors into the state being prepared by the circuit\n",
        "      # # #\n",
        "      qml.BasisState(np.zeros(self.wires), wires=self.dev_wires)\n",
        "      if self.layout=='basic_template':\n",
        "        shape = BasicEntanglerLayers.shape(n_layers=self.layers,\\\n",
        "                                                      n_wires=self.wires)\n",
        "        unitary_params=np.asarray(unitary_params).reshape(shape)\n",
        "        BasicEntanglerLayers(unitary_params,rotation=self.rotation,\\\n",
        "                                                        wires = self.dev_wires)\n",
        "      elif self.layout=='strong_template':\n",
        "        shape = StronglyEntanglingLayers.shape(n_layers=self.layers,\\\n",
        "                                                        n_wires=self.wires)\n",
        "        unitary_params=np.asarray(unitary_params).reshape(shape)\n",
        "        StronglyEntanglingLayers(unitary_params, wires = self.dev_wires)\n",
        "      elif self.layout=='two_design_template':\n",
        "        shape = SimplifiedTwoDesign.shape(n_layers=self.layers, \\\n",
        "                                                        n_wires=self.wires)\n",
        "        unitary_params=np.asarray(unitary_params).reshape(shape)\n",
        "        SimplifiedTwoDesign(unitary_params, wires = self.dev_wires)\n",
        "      elif self.layout=='user_defined_template':\n",
        "        # ADD YOUR CODE HERE\n",
        "        # # #\n",
        "        # write a function that can be passed to qml.template or use other\n",
        "        # functionalitiy such as qml.broadcast to build your own parameterized\n",
        "        # circuit design\n",
        "        # # #\n",
        "        raise NotImplementedError\n",
        "      else:\n",
        "        raise ValueError('Please choose a valid layout')\n",
        "      if final_params is not None:\n",
        "        # ADD YOUR CODE HERE\n",
        "        # Ensure that final_params has the correct shape\n",
        "        final_params = final_params.reshape((2,-1))\n",
        "        qml.broadcast(unitary=qml.RY, pattern=\"single\", \\\n",
        "                            wires=self.dev_wires, parameters=final_params[0])\n",
        "        qml.broadcast(unitary=qml.RX, pattern=\"single\", \\\n",
        "                      wires=self.dev_wires, parameters=final_params[1])\n",
        "\n",
        "      if self.prob_vec:\n",
        "          if self.pBF!=None:\n",
        "              for idx in self.dev_wires:\n",
        "                  qml.BitFlip(self.pBF, wires=self.dev_wires[int(idx)])\n",
        "              return qml.probs(wires=self.dev_wires)\n",
        "          return qml.probs(wires=self.dev_wires)\n",
        "      else:\n",
        "          return np.asarray([qml.expval(qml.PauliZ(i)) for i in self.dev_wires])\n",
        "\n",
        "    def draw_circuit(self,*args, **kwds):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def pdf(self,params,x=None):\n",
        "        '''\n",
        "        pennylane returns the probabilities in an OrderedDict - no need for sorting the values\n",
        "        '''\n",
        "        self.prob_vec=True\n",
        "        if x is None:\n",
        "            x = np.zeros(self.wires)\n",
        "        qnode_ = qml.QNode(self.build_circuit, self.device)\n",
        "        Q = qnode_(params,x)\n",
        "        return Q\n",
        "\n",
        "\n",
        "    def prediction(self,params,x=None):\n",
        "        '''\n",
        "        ADD This functionality:  given x and a parameter vector, return the predicted\n",
        "        image from the model\n",
        "        '''\n",
        "        # ADD CODE HERE\n",
        "        raise NotImplementedError\n",
        "        return\n",
        "\n",
        "    def spin_vector(self,params,x=None):\n",
        "      \"\"\" ADD CODE HERE \"\"\"\n",
        "      # Add code so that the output of the QCBM is a vector of [<Z0>, <Z1>,...]\n",
        "      raise NotImplementedError\n",
        "      return\n",
        "\n",
        "    def sample(self,*args,**kwds):\n",
        "      \"\"\" ADD CODE HERE\"\"\"\n",
        "      # Add code to prepare the pdf and draw N samples\n",
        "      # return samples as bitstrings s\n",
        "      raise NotImplementedError\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        return 1/(1+np.exp(-x))\n",
        "\n",
        "    def cost_function(self,params, x, y):\n",
        "      \"\"\"Cost function to be minimized.\n",
        "\n",
        "      Args:\n",
        "          params (array[float]): array of parameters\n",
        "          x (array[float]): 2-d array of input vectors\n",
        "          y (array[float]): 1-d array of targets\n",
        "          state_labels (array[float]): array of state representations for labels\n",
        "\n",
        "      Returns:\n",
        "          float: loss value to be minimized\n",
        "      \"\"\"\n",
        "\n",
        "      # This function requires the prediction function above\n",
        "      pred = self.prediction(params,x)\n",
        "      loss =np.mean((y - pred) ** 2)\n",
        "      return loss\n",
        "\n",
        "    def fit(self,startval,X,y,opt_method,nsteps=10,alpha=0.05):\n",
        "      \"\"\" ADD CODE HERE \"\"\"\n",
        "      # Add code to handle the case that an argument is missing\n",
        "\n",
        "      opt = opt_method(stepsize=alpha)\n",
        "\n",
        "      params = startval.copy()\n",
        "\n",
        "      loss = {}\n",
        "      # if cost_function is implementable, this returns the mean cost evaluated\n",
        "      # at the start of training\n",
        "      loss[0]=np.mean(np.sum([self.cost_function(params,X[i],y[i]) for i in range(len(X))]))\n",
        "      print('initial loss: ',loss[0])\n",
        "\n",
        "      # Option:  if you have a function to make predictions coded up\n",
        "      # preds = self.make_predictions(params,X)\n",
        "\n",
        "      stored_params = {}\n",
        "      stored_params[0]=params.copy()\n",
        "\n",
        "      for i in range(nsteps):\n",
        "          # Update the circuit parameters -- this implements supervised learning\n",
        "          # using online learning: the parameters are updated based on a gradient\n",
        "          # step defined by the loss evaluated at a single sample-label pair\n",
        "          # OPTIONAL:\n",
        "          # Modify this to use all samples, or batches of samples\n",
        "          for j in range(len(X)):\n",
        "            params = opt.step(lambda v: self.cost_function(v, X[j], y[j]), params)\n",
        "            self.coefs_ = params.copy()\n",
        "            # Option: Does this print out too much during training?  add a verbose keyword to toggle that on/off\n",
        "          if i%2==0:\n",
        "            _loss = np.mean(np.sum([self.cost_function(params,X[i],y[i]) for i in range(len(X))]))\n",
        "            print(i+1,_loss)\n",
        "          loss[i+1]=_loss.copy()\n",
        "          stored_params[i+1]=params.copy()\n",
        "      return params,loss,stored_params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "froIXwl24B4A"
      },
      "outputs": [],
      "source": [
        "n_wires = 4\n",
        "n_layers = 2\n",
        "n_shots = 1024\n",
        "my_qcbm = StyleQCBM(wires=n_wires,layers=n_layers,shots=n_shots,backend='sim',add_final_layer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H95MlH7W4HRD"
      },
      "outputs": [],
      "source": [
        "thetas = (np.pi)*np.random.random(n_layers*n_wires+2*n_wires*my_qcbm.add_final_layer)-np.pi/2."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visually inspect the circuit -- is it encoding the `x` feature as you expect?"
      ],
      "metadata": {
        "id": "FQXTTmw9tl2z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vr58zFE04PUA"
      },
      "outputs": [],
      "source": [
        "temp_qnode = qml.QNode(my_qcbm.build_circuit, my_qcbm.device)\n",
        "qml.draw_mpl(temp_qnode,decimals=3,expansion_strategy='device')(thetas,x=np.array([0,1,0.5,1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vcqiQBrDgAD"
      },
      "outputs": [],
      "source": [
        "my_qcbm.prediction(thetas,x=np.array([1,0,0.5,1])  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY0guEsrCfvl"
      },
      "outputs": [],
      "source": [
        "noisy_samples = generate_multiple_noisy_samples(2,15,0.5)\n",
        "X,y = build_noisy_clean_pairs(noisy_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, look at the generate dataset, verify it is properly created"
      ],
      "metadata": {
        "id": "x3fTSyvSuzES"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25pxRRzWSQzg"
      },
      "outputs": [],
      "source": [
        "np.round(X,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUEsirFp4YvQ"
      },
      "outputs": [],
      "source": [
        "trained_model,loss_curve,all_params = my_qcbm.fit(thetas,X,y,qml.AdamOptimizer,nsteps=20,alpha=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g=sns.lineplot(loss_curve.values(),legend=False)\n",
        "g.set(yscale='log')"
      ],
      "metadata": {
        "id": "c00k2oFjt-ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional:  Compare images\n",
        "\n",
        "1) Import `metrics` from `scikit-learn` and use to compare similarities between predictions and the known ground truth image.\n",
        "\n",
        "2)  Use `matplotlib` to generate plots comparing the noisy, generated output, and the ground truth"
      ],
      "metadata": {
        "id": "0gUOJOVberL_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRCMrWMc7O_J"
      },
      "outputs": [],
      "source": [
        "trained_model_predictions = []\n",
        "for idx in range(len(X)):\n",
        "  trained_model_predictions.append(my_qcbm.prediction(trained_model,X[idx]))\n",
        "trained_model_predictions.append(y[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kr4Um2dlzXu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC75zuJVbAvg"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-xgESLZbIMg"
      },
      "outputs": [],
      "source": [
        "M = metrics.pairwise.cosine_similarity(trained_model_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(M)"
      ],
      "metadata": {
        "id": "nx2POoYoxTqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(nrows=1, ncols=3,figsize=(10,10))\n",
        "n = 2\n",
        "idx = 2\n",
        "axs[0].imshow(np.array(X[idx]).reshape((n,n)).numpy(),cmap='coolwarm',vmin=0,vmax=1)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        text = axs[0].text(j, i, np.round(np.array(X[idx]).reshape((n,n)).numpy()[i, j],3),\n",
        "                       ha=\"center\", va=\"center\", color=\"w\")\n",
        "axs[1].imshow(np.array(trained_model_predictions[idx]).reshape((n,n)).numpy(),cmap='coolwarm',vmin=0,vmax=1)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        text = axs[1].text(j, i, np.round(np.array(trained_model_predictions[idx]).reshape((n,n)).numpy()[i, j],3),\n",
        "                       ha=\"center\", va=\"center\", color=\"w\")\n",
        "axs[2].imshow(np.array(y[idx]).reshape((n,n)).numpy(),cmap='coolwarm',vmin=0,vmax=1)\n",
        "for i in range(2):\n",
        "    for j in range(2):\n",
        "        text = axs[2].text(j, i, np.round(np.array(y[idx]).reshape((n,n)).numpy()[i, j],3),\n",
        "                       ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "for ax in axs.flatten():\n",
        "  # Major ticks\n",
        "  ax.set_xticks(np.arange(0, n, 1))\n",
        "  ax.set_yticks(np.arange(0, n, 1))\n",
        "\n",
        "  # Labels for major ticks\n",
        "  #ax.set_xticklabels(np.arange(1, n+1, 1))\n",
        "  #ax.set_yticklabels(np.arange(1, n+1, 1))\n",
        "\n",
        "  # Minor ticks\n",
        "  ax.set_xticks(np.arange(-.5, n, 1), minor=True)\n",
        "  ax.set_yticks(np.arange(-.5, n, 1), minor=True)\n",
        "\n",
        "  # Gridlines based on minor ticks\n",
        "  ax.grid(which='minor', color='w', linestyle='-', linewidth=2)\n",
        "\n",
        "  # Remove minor ticks\n",
        "  ax.tick_params(which='both', bottom=False, left=False)\n",
        "  ax.tick_params(which='major', bottom=False, left=False)\n",
        "fig.tight_layout()\n"
      ],
      "metadata": {
        "id": "_vhMIBYZxUxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional (Next Steps)\n",
        "\n",
        "1. How few pairs of (clean, noisy) images can be used to train the model?\n",
        "\n",
        "2. What is the maximum level of noise that the model can learn under?\n",
        "\n",
        "3. Try training a model with multiple unique images\n",
        "\n",
        "4. This model was trained under the assumption that the images were corrupted, but the qubits and circuit are noiseless.  How does adding quantum noise affect this model?  How noisy can the `StyleQCBM` be, before it no longer can be trained as an image denoiser?"
      ],
      "metadata": {
        "id": "UA1ltOc91PAC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZiCthcvP0CA7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}